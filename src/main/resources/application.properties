# URL base de tu instancia local de Ollama (el puerto por defecto)
spring.ai.ollama.base-url=http://localhost:11434 

# Modelo que usarás. Debe ser un modelo que ya hayas descargado en Ollama.
spring.ai.ollama.chat.options.model=mistral:latest